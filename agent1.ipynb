{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3c9169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spade_bdi.bdi import BDIAgent\n",
    "from transformers import pipeline\n",
    "user_score = []\n",
    "\n",
    "toxic = pipeline(\"text-classification\", model=\"koalaAI/Text-Moderation\", return_all_scores=True)\n",
    "\n",
    "\n",
    "def add_score(sender, text, score):\n",
    "    entry = {\n",
    "        \"sender\": sender,\n",
    "        \"text\": text,\n",
    "        \"score\": score\n",
    "    }\n",
    "    user_score.append(entry)\n",
    "class thread_moderator(BDIAgent):\n",
    "    def __init__(self, jid, password):\n",
    "        super().__init__(jid, password)\n",
    "        self.beliefs ={}\n",
    "        self.goals = []\n",
    "\n",
    "    async def setup(self):\n",
    "        self.beliefs[\"harmful\"] = {}\n",
    "        self.beliefs[\"user_score\"] = {}\n",
    "        self.set_internal_action(\"python_analyze\", self.python_analyze)\n",
    "\n",
    "        @actions.add_function(\".textScore\",(str, str) )\n",
    "        def textScore(body, sender):\n",
    "\n",
    "            results = toxic(body)\n",
    "            entries = results[0] if isinstance(results, list) and isinstance(results[0], list) else results\n",
    "            for item in entries:\n",
    "              if item['label'] == 'OK':\n",
    "                value = item['score']\n",
    "            msg_id = f\"{hash(body)}_{sender}\"\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703f9fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
